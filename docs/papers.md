---
layout: default
comments: false
keywords:

title: Papers
description:
buttons:
micro_nav: false
---

## Papers

<table id="schedule" class="table table-bordered no-more-tables" style="width: 100%; font-size: 0.8em;">
    <colgroup>
        <col style="width: 60%;">
        <col style="width: 40%;">
    </colgroup>
    <thead class="active" style="background-color:#f9f9f9" align="left">
        <th>Paper</th>
        <th>Description</th>
    </thead>
    <tbody>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>SVM</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1306.0239">Deep Learning using Linear Support Vector Machines</a></td>
            <td>claiming that the L2SVM outperforms Softmax</td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Softmax</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/pdf/1310.4546.pdf">Hierarchical Softmax</a></td>
            <td>large category</td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Autograd</strong>
            </td>
        </tr>
    	<tr>
            <td><a href="http://arxiv.org/abs/1306.0239">Automatic differentiation in machine learning</a></td>
            <td>backpropagation</td>
        </tr>
		<tr>
            <td><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient BackProp</a></td>
            <td>Efficient BackProp from Yann LeCun</td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Activation</strong>
            </td>
        </tr>
		<tr>
            <td><a href="http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf">Approximation by Superpositions of Sigmoidal Function</a></td>
            <td>universal approximators</td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></td>
            <td>RELU, RELU weights initialization</td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Deep Neural Networks</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1312.6184">Do Deep Nets Really Need to be Deep?</a></td>
            <td></td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1412.6550">FitNets: Hints for Thin Deep Nets</a></td>
            <td></td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1412.0233">The Loss Surfaces of Multilayer Networks</a></td>
            <td></td>
        </tr>
		<tr>
            <td><a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a></td>
            <td>weights initialization</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1502.03167">Batch Normalization</a></td>
            <td></td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Regularization</strong>
            </td>
        </tr>
        <tr>
            <td><a href="https://web.stanford.edu/~hastie/Papers/elasticnet.pdf">Elastic net regularization</a></td>
            <td>L1 regularization, L2 regularization</td>
        </tr>
		<tr>
            <td><a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></td>
            <td>dropout</td>
        </tr>
		<tr>
            <td><a href="http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf">Dropout Training as Adaptive Regularization</a></td>
            <td>dropout relation to the other regularization techniques</td>
        </tr>
		<tr>
            <td><a href="">DropConnect</a></td>
            <td></td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>VGG</strong>
            </td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></td>
            <td>vgg net</td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Optimization</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/pdf/1212.0901v2.pdf">Advances in optimizing Recurrent Networks</a></td>
            <td>Nesterov Momentum</td>
        </tr>
		<tr>
            <td><a href="http://research.google.com/archive/large_deep_networks_nips2012.html">Large Scale Distributed Deep Networks</a></td>
            <td>comparing L-BFGS and SGD variants</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1311.2115">SFO</a></td>
            <td>combine the advantages of SGD with advantages of L-BFGS</td>
        </tr>
		<tr>
            <td><a href="http://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></td>
            <td>Adagrad - adaptive learning rate method</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a></td>
            <td>Adam</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1312.6055">Unit Tests for Stochastic Optimization</a></td>
            <td>a series of tests as a standardized benchmark for stochastic optimization</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/pdf/1206.5533v2.pdf">Practical Recommendations for Gradient-Based Training of Deep Architectures</a></td>
            <td></td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>HyperParameter Search</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a></td>
            <td></td>
        </tr>
    </tbody>
</table>
