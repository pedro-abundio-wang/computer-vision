---
layout: default
comments: false
keywords:

title: Papers
description:
buttons:
micro_nav: false
---

## Papers

<table id="schedule" class="table table-bordered no-more-tables" style="width: 100%; font-size: 0.8em;">
    <colgroup>
        <col style="width: 60%;">
        <col style="width: 40%;">
    </colgroup>
    <thead class="active" style="background-color:#f9f9f9" align="left">
        <th>Paper</th>
        <th>Description</th>
    </thead>
    <tbody>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>SVM</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1306.0239">Deep Learning using Linear Support Vector Machines</a></td>
            <td>claiming that the L2SVM outperforms Softmax</td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Softmax</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/pdf/1310.4546.pdf">Hierarchical Softmax</a></td>
            <td>large category</td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Autograd</strong>
            </td>
        </tr>
    	<tr>
            <td><a href="http://arxiv.org/abs/1306.0239">Automatic differentiation in machine learning</a></td>
            <td>backpropagation</td>
        </tr>
		<tr>
            <td><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient BackProp</a></td>
            <td>Efficient BackProp from Yann LeCun</td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Activation</strong>
            </td>
        </tr>
		<tr>
            <td><a href="http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf">Approximation by Superpositions of Sigmoidal Function</a></td>
            <td>universal approximators</td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></td>
            <td>RELU, RELU weights initialization</td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Deep Neural Networks</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1312.6184">Do Deep Nets Really Need to be Deep?</a></td>
            <td></td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1412.6550">FitNets: Hints for Thin Deep Nets</a></td>
            <td></td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1412.0233">The Loss Surfaces of Multilayer Networks</a></td>
            <td></td>
        </tr>
		<tr>
            <td><a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a></td>
            <td>weights initialization</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></td>
            <td></td>
        </tr>
		<tr>
            <td><a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></td>
            <td>Instead of normalizing over the batch, we normalize over the features</td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Regularization</strong>
            </td>
        </tr>
        <tr>
            <td><a href="https://web.stanford.edu/~hastie/Papers/elasticnet.pdf">Elastic net regularization</a></td>
            <td>L1 regularization, L2 regularization</td>
        </tr>
		<tr>
            <td><a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></td>
            <td>dropout</td>
        </tr>
		<tr>
            <td><a href="http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf">Dropout Training as Adaptive Regularization</a></td>
            <td>dropout relation to the other regularization techniques</td>
        </tr>
		<tr>
            <td><a href="">DropConnect</a></td>
            <td></td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Optimization</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/pdf/1212.0901v2.pdf">Advances in optimizing Recurrent Networks</a></td>
            <td>Nesterov Momentum</td>
        </tr>
		<tr>
            <td><a href="http://research.google.com/archive/large_deep_networks_nips2012.html">Large Scale Distributed Deep Networks</a></td>
            <td>comparing L-BFGS and SGD variants</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1311.2115">SFO</a></td>
            <td>combine the advantages of SGD with advantages of L-BFGS</td>
        </tr>
		<tr>
            <td><a href="http://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></td>
            <td>Adagrad - adaptive learning rate method</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a></td>
            <td>Adam</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1312.6055">Unit Tests for Stochastic Optimization</a></td>
            <td>a standardized benchmark for stochastic optimization</td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/pdf/1206.5533v2.pdf">Practical Recommendations for Gradient-Based Training of Deep Architectures</a></td>
            <td></td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>HyperParameter Search</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a></td>
            <td></td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Convolutional Neural Networks</strong>
            </td>
        </tr>
        <tr>
            <td><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">Gradient-Based Learning Applied to Document Recognition</a></td>
            <td>LeNet-5</td>
        </tr>
        <tr>
            <td><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">ImageNet Classification with Deep Convolutional Neural Networks</a></td>
            <td>AlexNet</td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolutional Networks</a></td>
            <td>ZF Net - Improvement on AlexNet</td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1409.4842">(Inception Network) Going Deeper with Convolutions</a></td>
            <td>GoogLeNet</td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1602.07261">Inception-ResNet and the Impact of Residual Connections on Learning</a></td>
            <td>Inception-v4</td>
        </tr>
        <tr>
            <td><a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></td>
            <td>VGGNet</td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1512.03385">(ResNet) Deep residual networks for image recognition</a></td>
            <td>Residual Network</td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1603.05027">Identity Mappings in Deep Residual Networks</a></td>
            <td></td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1312.4400">Network in Network</a></td>
            <td></td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1511.07122">Multi-Scale Context Aggregation by Dilated Convolutions</a></td>
            <td>Dilated convolutions</td>
        </tr>
        <tr>
            <td><a href="http://arxiv.org/abs/1412.6806">Striving for Simplicity: The All Convolutional Net</a></td>
            <td></td>
        </tr>
    </tbody>
</table>
