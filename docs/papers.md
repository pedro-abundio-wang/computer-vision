---
layout: default
comments: false
keywords:

title: Papers
description:
buttons:
micro_nav: false
---

## Papers

<table id="schedule" class="table table-bordered no-more-tables" style="width: 100%; font-size: 0.8em;">
    <colgroup>
        <col style="width: 60%;">
        <col style="width: 40%;">
    </colgroup>
    <thead class="active" style="background-color:#f9f9f9" align="left">
        <th>Paper</th>
        <th>Description</th>
    </thead>
    <tbody>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>SVM</strong>
            </td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1306.0239">Deep Learning using Linear Support Vector Machines</a></td>
            <td>claiming that the L2SVM outperforms Softmax.</td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Autograd</strong>
            </td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1306.0239">Automatic differentiation in machine learning</a></td>
            <td>backpropagation</td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Universal Approximators</strong>
            </td>
        </tr>
		<tr>
            <td><a href="http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf">Approximation by Superpositions of Sigmoidal Function</a></td>
            <td>universal approximators</td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Activation</strong>
            </td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers</a></td>
            <td>backpropagation</td>
        </tr>
		<tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Depth of Deep Neural Networks</strong>
            </td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1312.6184">Do Deep Nets Really Need to be Deep?</a></td>
            <td></td>
        </tr>
		<tr>
            <td><a href="http://arxiv.org/abs/1412.6550">FitNets: Hints for Thin Deep Nets</a></td>
            <td></td>
        </tr>
        <tr>
            <td id="" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>VGG</strong>
            </td>
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></td>
            <td>vgg net</td>
        </tr>
    </tbody>
</table>
