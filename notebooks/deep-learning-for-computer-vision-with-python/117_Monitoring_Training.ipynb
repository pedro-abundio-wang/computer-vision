{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pedro/miniconda3/envs/computer-vision/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pedro/miniconda3/envs/computer-vision/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pedro/miniconda3/envs/computer-vision/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pedro/miniconda3/envs/computer-vision/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pedro/miniconda3/envs/computer-vision/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pedro/miniconda3/envs/computer-vision/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.callbacks import BaseLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor(BaseLogger):\n",
    "    \n",
    "    def __init__(self, figPath, jsonPath=None, startAt=0):\n",
    "        # store the output path for the figure, the path to the JSON\n",
    "        # serialized file, and the starting epoch\n",
    "        super(TrainingMonitor, self).__init__()\n",
    "        self.figPath = figPath\n",
    "        self.jsonPath = jsonPath\n",
    "        self.startAt = startAt\n",
    "        \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        # initialize the history dictionary\n",
    "        self.H = {}\n",
    "        \n",
    "        # if the JSON history path exists, load the training history\n",
    "        if self.jsonPath is not None:\n",
    "            if os.path.exists(self.jsonPath):\n",
    "                self.H = json.loads(open(self.jsonPath).read())\n",
    "                \n",
    "                # check to see if a starting epoch was supplied\n",
    "                if self.startAt > 0:\n",
    "                    # loop over the entries in the history log and\n",
    "                    # trim any entries that are past the starting epoch\n",
    "                    for k in self.H.keys():\n",
    "                        self.H[k] = self.H[k][:self.startAt]\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # loop over the logs and update the loss, accuracy, etc.\n",
    "        # for the entire training process\n",
    "        for (k, v) in logs.items():\n",
    "            l = self.H.get(k, [])\n",
    "            l.append(v)\n",
    "            self.H[k] = l\n",
    "            \n",
    "        # check to see if the training history should be serialized to file\n",
    "        if self.jsonPath is not None:\n",
    "            f = open(self.jsonPath, \"w\")\n",
    "            f.write(json.dumps(self.H))\n",
    "            f.close()\n",
    "            \n",
    "        # ensure at least two epochs have passed before plotting\n",
    "        # (epoch starts at zero)\n",
    "        if len(self.H[\"loss\"]) > 1:\n",
    "            # plot the training loss and accuracy\n",
    "            N = np.arange(0, len(self.H[\"loss\"]))\n",
    "            plt.style.use(\"ggplot\")\n",
    "            plt.figure()\n",
    "            plt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n",
    "            plt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n",
    "            plt.plot(N, self.H[\"acc\"], label=\"train_acc\")\n",
    "            plt.plot(N, self.H[\"val_acc\"], label=\"val_acc\")\n",
    "            plt.title(\"Training Loss and Accuracy [Epoch {}]\".format(len(self.H[\"loss\"])))\n",
    "            plt.xlabel(\"Epoch #\")\n",
    "            plt.ylabel(\"Loss/Accuracy\")\n",
    "            plt.legend()\n",
    "            \n",
    "            # save the figure\n",
    "            plt.savefig(self.figPath)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import cifar10\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO process ID: 17588\n"
     ]
    }
   ],
   "source": [
    "# show information on the process ID\n",
    "print(\"[INFO process ID: {}\".format(os.getpid()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading CIFAR-10 data...\n"
     ]
    }
   ],
   "source": [
    "# load the training and testing data, then scale it into the\n",
    "# range [0, 1]\n",
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\") / 255.0\n",
    "testX = testX.astype(\"float\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels from integers to vectors\n",
    "labelBinarizer = LabelBinarizer()\n",
    "trainY = labelBinarizer.fit_transform(trainY)\n",
    "testY = labelBinarizer.fit_transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the label names for the CIFAR-10 dataset\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "from classifiers.mini_vgg import MiniVGGNet\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "miniVGG = MiniVGGNet()\n",
    "model = miniVGG.build(width=32, height=32, depth=3, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the set of callbacks\n",
    "figPath = os.path.sep.join([\"output/monitoring_training\", \"{}.png\".format(os.getpid())])\n",
    "jsonPath = os.path.sep.join([\"output/monitoring_training\", \"{}.json\".format(os.getpid())])\n",
    "callbacks = [TrainingMonitor(figPath, jsonPath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 1.1538 - acc: 0.6037 - val_loss: 1.0465 - val_acc: 0.6389\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.9839 - acc: 0.6606 - val_loss: 1.0272 - val_acc: 0.6480\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.8752 - acc: 0.6960 - val_loss: 0.9593 - val_acc: 0.6735\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.7987 - acc: 0.7230 - val_loss: 0.7392 - val_acc: 0.7437\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.7362 - acc: 0.7429 - val_loss: 0.6734 - val_acc: 0.7674\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.6876 - acc: 0.7596 - val_loss: 0.7343 - val_acc: 0.7499\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.6451 - acc: 0.7741 - val_loss: 0.8002 - val_acc: 0.7356\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.6029 - acc: 0.7891 - val_loss: 0.6355 - val_acc: 0.7838\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.5700 - acc: 0.7995 - val_loss: 0.6349 - val_acc: 0.7848\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.5367 - acc: 0.8120 - val_loss: 0.6039 - val_acc: 0.7945\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.4973 - acc: 0.8234 - val_loss: 0.6268 - val_acc: 0.7940\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.4731 - acc: 0.8312 - val_loss: 0.6023 - val_acc: 0.7975\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.4497 - acc: 0.8403 - val_loss: 0.6116 - val_acc: 0.8011\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.4316 - acc: 0.8483 - val_loss: 0.5559 - val_acc: 0.8123\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.4019 - acc: 0.8571 - val_loss: 0.5760 - val_acc: 0.8136\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.3883 - acc: 0.8620 - val_loss: 0.5860 - val_acc: 0.8125\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.3675 - acc: 0.8675 - val_loss: 0.5936 - val_acc: 0.8054\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.3482 - acc: 0.8752 - val_loss: 0.6005 - val_acc: 0.8057\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.3320 - acc: 0.8827 - val_loss: 0.6481 - val_acc: 0.8016\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.3415 - acc: 0.8792 - val_loss: 0.5866 - val_acc: 0.8152\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.3087 - acc: 0.8909 - val_loss: 0.7062 - val_acc: 0.7896\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.2990 - acc: 0.8930 - val_loss: 0.6345 - val_acc: 0.8066\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.2839 - acc: 0.8976 - val_loss: 0.6094 - val_acc: 0.8135\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.2684 - acc: 0.9032 - val_loss: 0.6549 - val_acc: 0.8105\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.2641 - acc: 0.9061 - val_loss: 0.5845 - val_acc: 0.8219\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.2495 - acc: 0.9118 - val_loss: 0.6170 - val_acc: 0.8128\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.2470 - acc: 0.9116 - val_loss: 0.6137 - val_acc: 0.8099\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.2383 - acc: 0.9150 - val_loss: 0.5901 - val_acc: 0.8250\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.2283 - acc: 0.9187 - val_loss: 0.6112 - val_acc: 0.8186\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.2211 - acc: 0.9217 - val_loss: 0.6040 - val_acc: 0.8258\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.2095 - acc: 0.9259 - val_loss: 0.5971 - val_acc: 0.8242\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.2083 - acc: 0.9263 - val_loss: 0.6372 - val_acc: 0.8220\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1949 - acc: 0.9298 - val_loss: 0.5856 - val_acc: 0.8286\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.1995 - acc: 0.9291 - val_loss: 0.5992 - val_acc: 0.8269\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1895 - acc: 0.9326 - val_loss: 0.6057 - val_acc: 0.8270\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1894 - acc: 0.9324 - val_loss: 0.6385 - val_acc: 0.8226\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1837 - acc: 0.9349 - val_loss: 0.6558 - val_acc: 0.8160\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1713 - acc: 0.9393 - val_loss: 0.5992 - val_acc: 0.8286\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1711 - acc: 0.9388 - val_loss: 0.6679 - val_acc: 0.8174\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1733 - acc: 0.9385 - val_loss: 0.6239 - val_acc: 0.8260\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1617 - acc: 0.9431 - val_loss: 0.6697 - val_acc: 0.8211\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1624 - acc: 0.9416 - val_loss: 0.6417 - val_acc: 0.8257\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1598 - acc: 0.9433 - val_loss: 0.6358 - val_acc: 0.8268\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1527 - acc: 0.9455 - val_loss: 0.6599 - val_acc: 0.8173\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1503 - acc: 0.9464 - val_loss: 0.6618 - val_acc: 0.8178\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1634 - acc: 0.9415 - val_loss: 0.6378 - val_acc: 0.8274\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1459 - acc: 0.9475 - val_loss: 0.6255 - val_acc: 0.8277\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1437 - acc: 0.9495 - val_loss: 0.7067 - val_acc: 0.8110\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1518 - acc: 0.9461 - val_loss: 0.6246 - val_acc: 0.8294\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1410 - acc: 0.9496 - val_loss: 0.6268 - val_acc: 0.8342\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1381 - acc: 0.9505 - val_loss: 0.6488 - val_acc: 0.8303\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1347 - acc: 0.9525 - val_loss: 0.6479 - val_acc: 0.8300\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1333 - acc: 0.9529 - val_loss: 0.6300 - val_acc: 0.8312\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 0.1265 - acc: 0.9543 - val_loss: 0.6343 - val_acc: 0.8351\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1301 - acc: 0.9538 - val_loss: 0.6764 - val_acc: 0.8290\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1255 - acc: 0.9553 - val_loss: 0.6355 - val_acc: 0.8287\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1219 - acc: 0.9567 - val_loss: 0.6511 - val_acc: 0.8361\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1233 - acc: 0.9564 - val_loss: 0.6406 - val_acc: 0.8301\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1173 - acc: 0.9582 - val_loss: 0.6230 - val_acc: 0.8313\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 72s 1ms/step - loss: 0.1139 - acc: 0.9595 - val_loss: 0.6644 - val_acc: 0.8315\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1131 - acc: 0.9607 - val_loss: 0.6520 - val_acc: 0.8302\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1117 - acc: 0.9597 - val_loss: 0.6362 - val_acc: 0.8301\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1128 - acc: 0.9606 - val_loss: 0.6564 - val_acc: 0.8331\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1144 - acc: 0.9603 - val_loss: 0.6607 - val_acc: 0.8333\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1033 - acc: 0.9637 - val_loss: 0.6669 - val_acc: 0.8318\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1059 - acc: 0.9630 - val_loss: 0.6772 - val_acc: 0.8257\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1039 - acc: 0.9633 - val_loss: 0.6371 - val_acc: 0.8342\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1058 - acc: 0.9625 - val_loss: 0.6580 - val_acc: 0.8322\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.1036 - acc: 0.9639 - val_loss: 0.6618 - val_acc: 0.8288\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 72s 1ms/step - loss: 0.1031 - acc: 0.9639 - val_loss: 0.6883 - val_acc: 0.8311\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1033 - acc: 0.9647 - val_loss: 0.7476 - val_acc: 0.8200\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0999 - acc: 0.9640 - val_loss: 0.6975 - val_acc: 0.8330\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.1022 - acc: 0.9644 - val_loss: 0.6731 - val_acc: 0.8327\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.0964 - acc: 0.9656 - val_loss: 0.6586 - val_acc: 0.8348\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0999 - acc: 0.9643 - val_loss: 0.6564 - val_acc: 0.8350\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0951 - acc: 0.9665 - val_loss: 0.6606 - val_acc: 0.8368\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0947 - acc: 0.9676 - val_loss: 0.6709 - val_acc: 0.8311\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0983 - acc: 0.9658 - val_loss: 0.6571 - val_acc: 0.8334\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0956 - acc: 0.9662 - val_loss: 0.6758 - val_acc: 0.8343\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0915 - acc: 0.9681 - val_loss: 0.6608 - val_acc: 0.8361\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.0915 - acc: 0.9682 - val_loss: 0.7326 - val_acc: 0.8297\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0886 - acc: 0.9686 - val_loss: 0.6497 - val_acc: 0.8340\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0863 - acc: 0.9701 - val_loss: 0.6483 - val_acc: 0.8378\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0902 - acc: 0.9688 - val_loss: 0.6947 - val_acc: 0.8302\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0874 - acc: 0.9690 - val_loss: 0.6676 - val_acc: 0.8351\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.0896 - acc: 0.9688 - val_loss: 0.6713 - val_acc: 0.8331\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0868 - acc: 0.9701 - val_loss: 0.6780 - val_acc: 0.8320\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0819 - acc: 0.9715 - val_loss: 0.6799 - val_acc: 0.8308\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0846 - acc: 0.9702 - val_loss: 0.6971 - val_acc: 0.8314\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.0868 - acc: 0.9703 - val_loss: 0.6816 - val_acc: 0.8351\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0830 - acc: 0.9705 - val_loss: 0.7077 - val_acc: 0.8275\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.0789 - acc: 0.9718 - val_loss: 0.6854 - val_acc: 0.8352\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0820 - acc: 0.9720 - val_loss: 0.7310 - val_acc: 0.8226\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0818 - acc: 0.9708 - val_loss: 0.6578 - val_acc: 0.8358\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0793 - acc: 0.9724 - val_loss: 0.6715 - val_acc: 0.8353\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 70s 1ms/step - loss: 0.0801 - acc: 0.9720 - val_loss: 0.7054 - val_acc: 0.8382\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0824 - acc: 0.9720 - val_loss: 0.7068 - val_acc: 0.8360\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 72s 1ms/step - loss: 0.0776 - acc: 0.9728 - val_loss: 0.6717 - val_acc: 0.8353\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0754 - acc: 0.9737 - val_loss: 0.7238 - val_acc: 0.8368\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 71s 1ms/step - loss: 0.0747 - acc: 0.9739 - val_loss: 0.6652 - val_acc: 0.8344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1c323cd908>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=64, epochs=100, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
